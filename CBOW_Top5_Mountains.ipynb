{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlJHd7zl675MxrnFfUGQuB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjayBista1010/DeepLearning/blob/main/CBOW_Top5_Mountains.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWgsdCDOUpUj",
        "outputId": "aa086e1f-ff4b-4e1e-adb2-1ca932ae5b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched and processed text for Mount_Everest (length: 80998 chars)\n",
            "Fetched and processed text for K2 (length: 20121 chars)\n",
            "Fetched and processed text for Kangchenjunga (length: 9530 chars)\n",
            "Fetched and processed text for Lhotse (length: 4465 chars)\n",
            "Fetched and processed text for Makalu (length: 2544 chars)\n",
            "Number of documents in corpus: 5\n",
            "First 500 characters of first document:\n",
            " mount everest known locally as sagarm훮th훮a in nepal and qomolangmab in tibet is earths highest mount\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import string\n",
        "\n",
        "pages = [\"Mount_Everest\", \"K2\", \"Kangchenjunga\", \"Lhotse\", \"Makalu\"]\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                  \"Chrome/120.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "def extract_paragraphs(soup):\n",
        "    \"\"\"\n",
        "    Extract text from <p> tags only to avoid empty divs and tables\n",
        "    \"\"\"\n",
        "    paragraphs = soup.find_all(\"p\")\n",
        "    text_list = []\n",
        "    for p in paragraphs:\n",
        "        t = p.get_text().strip()\n",
        "        if t:\n",
        "            text_list.append(t)\n",
        "    return \"\\n\".join(text_list)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "corpus = []\n",
        "\n",
        "for page in pages:\n",
        "    url = f\"https://en.wikipedia.org/wiki/{page}\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        text = extract_paragraphs(soup)\n",
        "        processed_text = preprocess_text(text)\n",
        "        corpus.append(processed_text)\n",
        "        print(f\"Fetched and processed text for {page} (length: {len(processed_text)} chars)\")\n",
        "    else:\n",
        "        print(f\"Failed to fetch {page}, status code: {response.status_code}\")\n",
        "\n",
        "print(\"Number of documents in corpus:\", len(corpus))\n",
        "print(\"First 500 characters of first document:\\n\", corpus[0][:100])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = \" \".join(corpus).split()\n",
        "print(words[:10])\n",
        "vocab = sorted(set(words))\n",
        "word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "idx2word = {i: w for w,i in word2idx.items()}\n",
        "vocab_size = len(vocab)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mbhTgbvvR7n",
        "outputId": "530ccc8a-8f2e-426e-b6d3-727840b3fdb4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mount', 'everest', 'known', 'locally', 'as', 'sagarm훮th훮a', 'in', 'nepal', 'and', 'qomolangmab']\n",
            "4496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"vocab:\", vocab[:10])\n",
        "print(\"word2idx first 10 items:\", list(word2idx.items())[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcVzsCkTxAFi",
        "outputId": "1b367794-cb12-4ed0-9a89-4f462093e0c8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab: ['012', '016', '024', '0333', '04', '1', '10', '100', '1000', '10000']\n",
            "word2idx first 10 items: [('012', 0), ('016', 1), ('024', 2), ('0333', 3), ('04', 4), ('1', 5), ('10', 6), ('100', 7), ('1000', 8), ('10000', 9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [[word2idx[w] for w in s.split()] for s in corpus]\n",
        "sequences[0][:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yL_DMsxqxFvP",
        "outputId": "184a81d5-ae32-4f53-f18f-8cdb2cd9f24a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2814, 1665, 2460, 2595, 728, 3562, 2249, 2893, 661, 3311]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contexts = []\n",
        "targets = []\n",
        "window = 2"
      ],
      "metadata": {
        "id": "KMjlBhRbxOjV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#only considering positions that have both left and right words\n",
        "\n",
        "for seq in sequences:\n",
        "  for i in range(window, len(seq)-window):\n",
        "    context = [seq[i-1], seq[i+1]] #left and right\n",
        "    target = seq[i]\n",
        "    contexts.append(context)\n",
        "    targets.append(target)"
      ],
      "metadata": {
        "id": "hwNV14PxxRL-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contexts[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgjTqOY9xTFG",
        "outputId": "31e6a424-fdd5-42b5-f7bd-1702b62eb22c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1665, 2595],\n",
              " [2460, 728],\n",
              " [2595, 3562],\n",
              " [728, 2249],\n",
              " [3562, 2893],\n",
              " [2249, 661],\n",
              " [2893, 3311],\n",
              " [661, 2249],\n",
              " [3311, 4087],\n",
              " [2249, 2336]]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "yybdhSKbxYiZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contexts = torch.tensor(contexts, dtype=torch.long)\n",
        "print(contexts)\n",
        "targets = torch.tensor(targets, dtype=torch.long)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ns6BTrExUkh",
        "outputId": "ef20d9fe-17f3-4f84-cab7-2d972fc38bb2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1665, 2595],\n",
            "        [2460,  728],\n",
            "        [2595, 3562],\n",
            "        ...,\n",
            "        [ 864, 1846],\n",
            "        [ 508, 1860],\n",
            "        [1846, 4438]])\n",
            "tensor([2460, 2595,  728,  ...,  508, 1846, 1860])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOWDataset(Dataset):\n",
        "    def __init__(self, contexts, targets):\n",
        "        self.contexts = contexts\n",
        "        self.targets = targets\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.contexts[idx], self.targets[idx]"
      ],
      "metadata": {
        "id": "SyLDA2ONxcY3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CBOWDataset(contexts, targets)\n",
        "loader = DataLoader(dataset, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "Hi4L_h0Exfq-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.fc = nn.Linear(emb_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.emb(x)\n",
        "        mean = embeds.mean(dim=1)\n",
        "        out = self.fc(mean)\n",
        "        return out"
      ],
      "metadata": {
        "id": "r_C5p38wxh4e"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb_dim = 10\n",
        "model = CBOW(vocab_size=vocab_size, emb_dim=emb_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "9UtXmrwExjT2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 150\n",
        "for epoch in range(1, epochs + 1):\n",
        "    total_loss = 0.0\n",
        "    for ctx, tgt in loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(ctx)\n",
        "        loss = criterion(logits, tgt)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    if epoch % 30 == 0 or epoch == 1:\n",
        "        print(f\"Epoch {epoch:3d} - avg loss: {total_loss / len(loader):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp5U88F2xll3",
        "outputId": "06de69b2-be8d-4348-8898-f770d1a3a01c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   1 - avg loss: 7.4464\n",
            "Epoch  30 - avg loss: 5.1171\n",
            "Epoch  60 - avg loss: 5.0252\n",
            "Epoch  90 - avg loss: 4.9911\n",
            "Epoch 120 - avg loss: 4.9761\n",
            "Epoch 150 - avg loss: 4.9670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.emb.weight.detach().numpy()\n",
        "for i, (word, idx) in enumerate(word2idx.items()):\n",
        "    print(word, embeddings[idx][:5])\n",
        "    if i == 9:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MevCldwWxpYo",
        "outputId": "71db5f06-05da-47f7-eaf7-d91a671a84a4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "012 [ 2.8199644 -7.945237   7.2808814 -0.065623  -6.2410045]\n",
            "016 [ 0.6251435 -2.4418862 -1.9786686  1.2966774  6.1659393]\n",
            "024 [-7.959104   -7.7805476  -9.930698    0.20892972 20.084316  ]\n",
            "0333 [-2.2756548  6.964826   2.892752  -1.8399916  9.8171215]\n",
            "04 [-0.13834625 -6.3407974   5.1883273   6.5660815  19.906788  ]\n",
            "1 [ 1.9471714 -4.6404133  1.6328532 -2.6106403 -2.0116222]\n",
            "10 [ 3.0057287   0.49790943 -5.2661695  -3.9126866  -1.0671101 ]\n",
            "100 [-1.4478639  -1.9239297   1.9309886   0.63080233  4.4433093 ]\n",
            "1000 [ 1.105321  -3.4645805 -1.1251391  3.2168832 -3.0299826]\n",
            "10000 [-2.5030866  2.9571662  4.394929   5.1762705 -0.8749694]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(context_words):\n",
        "    idxs = torch.tensor([[word2idx[w] for w in context_words]], dtype=torch.long)  # [1,2]\n",
        "    with torch.no_grad():\n",
        "        logits = model(idxs)             # [1, vocab_size]\n",
        "        pred_idx = logits.argmax(dim=1).item()\n",
        "    return idx2word[pred_idx]"
      ],
      "metadata": {
        "id": "Y3VxodXSxqER"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"['mount', 'everest', 'is', 'the'] ->\", predict(['mount', 'everest', 'is', 'the']))\n",
        "print(\"['the', 'highest', 'mountain', 'in'] ->\", predict(['the', 'highest', 'mountain', 'in']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR9FPV8BxsRI",
        "outputId": "6667104f-f84c-44c5-c20f-9efe03e240e9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['mount', 'everest', 'is', 'the'] -> everest\n",
            "['the', 'highest', 'mountain', 'in'] -> the\n"
          ]
        }
      ]
    }
  ]
}